{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df4676d",
   "metadata": {},
   "source": [
    "# Experiment 4.3: Comparing clipping techniques\n",
    "\n",
    "The aim of this experiment is to find the best clipping technique for captioning with the BMT. The two techniques we analyze are fixed-time clipping and scene-based clipping. We will determine a technique is better than the other based on semantic similarity to ground-truths (METEOR score). In addition, other statistics such as average number of captions and CPM will be used to analyze the granularity of event detection. \n",
    "\n",
    "**The notebook from experiment 4.1 must be executed before this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686b00a",
   "metadata": {},
   "source": [
    "## 1. Enviornment\n",
    "\n",
    "In the following cell, change assign WD the path to the BMT-Clipping repository. \n",
    "Additionally, assign the path to your environments directory (e.g. where conda stores all directories) to the ENVS_PATH variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory (it must be the repository's root directory)\n",
    "WD = '/home/A01630791/BMT-Clipping'\n",
    "%cd $WD\n",
    "\n",
    "# Environments directory (e.g. anaconda3/envs)\n",
    "ENVS_PATH = '/home/A01630791/anaconda3/envs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0a27c",
   "metadata": {},
   "source": [
    "**Optional**  \n",
    "Uncomment and run the following cell if you haven't configured the Python environment in experiment 4.1\n",
    "\n",
    "Run directly in terminal if the notebook throws errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Environment\n",
    "## feature extraction (run directly in terminal if the notebook can't execute)\n",
    "!conda env create -f ./submodules/video_features/conda_env_i3d.yml\n",
    "!conda env create -f ./submodules/video_features/conda_env_vggish.yml\n",
    "## captioning model (run directly in terminal if the notebook can't execute)\n",
    "!conda env create -f ./conda_env.yml\n",
    "## spacy language model (use the path to your 'bmt' environment instead)\n",
    "!$ENVS_PATH/bmt/bin/python -m spacy download en\n",
    "\n",
    "\n",
    "# (Optional) Install additional libraries in environment (run directly in terminal if the notebook can't execute)\n",
    "!conda install pytube\n",
    "!conda install numpy\n",
    "!conda install matplotlib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5627db0",
   "metadata": {},
   "source": [
    "**BMT imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(WD)\n",
    "from sample.single_video_prediction import get_video_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025edb7f",
   "metadata": {},
   "source": [
    "**Module imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipping_modules.filtering import FilteringModule\n",
    "from clipping_modules.clipping import ClippingModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc33cc0",
   "metadata": {},
   "source": [
    "**Other imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e8c7b",
   "metadata": {},
   "source": [
    "## 2. Importing the dataset\n",
    "\n",
    "A copy of LABC is provided in our repository in `data/long_activitynet_categories.csv`, so there is no need to download it. The url is https://github.com/oscarmires/BMT-Clipping/blob/master/data/long_activitynet_categories.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the value of the following variable with the path to the dataset in your fs.\n",
    "LABC_PATH = f'{WD}/data/long_activitynet_categories.csv'\n",
    "LABC_PATH\n",
    "\n",
    "# Open the file as a Pandas DF\n",
    "labc_df = pd.read_csv(LABC_PATH, index_col=0)\n",
    "labc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labc_df.iloc[:20].duration.sum() / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14579407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: sort by duration\n",
    "labc_df.sort_values(by='duration', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab1fff",
   "metadata": {},
   "source": [
    "### 2.1 YouTube Downloads\n",
    "The following cell downloads from YouTube all the videos listed in LABC. Create a directory to store the videos. A path to this directory must be assigned to the variable `LABC_VIDEOS_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your path to the next variable\n",
    "LABC_VIDEOS_PATH = '/home/A01630791/bmt_clipping_experiments/LABC_videos'\n",
    "\n",
    "video_count = 0\n",
    "labc_rows = labc_df.shape[0]\n",
    "\n",
    "dir_list = os.listdir(LABC_VIDEOS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_id, sample in labc_df.iterrows():\n",
    "    video_count += 1\n",
    "    \n",
    "    if f'{video_id}.mp4' in dir_list:\n",
    "        print(f'\"{video_id}.mp4\" already in directory.')\n",
    "        continue\n",
    "    \n",
    "    try: \n",
    "        print(f'Downloading {video_count}/{labc_rows}...')\n",
    "        \n",
    "        yt = YouTube(sample['url']) \n",
    "        filename = f'{video_id}.mp4'\n",
    "        stream = yt.streams.get_highest_resolution()\n",
    "        stream.download(output_path=LABC_VIDEOS_PATH, filename=filename)\n",
    "    except: \n",
    "        print(\"Error: can't instantiate YouTube object.\") \n",
    "        exec_md['not_found_videos'].append(video_id)\n",
    "        os.remove(f'{LABC_VIDEOS_PATH}/{filename}')\n",
    "    # Optional break to limit number of videos:\n",
    "    if video_count >= 25:\n",
    "        break\n",
    "        \n",
    "print(f'Downloaded {video_count}/{labc_rows} videos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe961b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LABC_VIDEOS_PATH = '/home/A01630791/bmt_clipping_experiments/Output_4_3/ft_clips'\n",
    "\n",
    "\n",
    "df = pd.DataFrame(os.listdir(LABC_VIDEOS_PATH))\n",
    "df['clip_id'] = [(int) (re.findall(\"(?<=&).*?(?=\\.)\", row[0])[0]) for index, row in df.iterrows()]\n",
    "\n",
    "df.sort_values(by='clip_id')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71156d5",
   "metadata": {},
   "source": [
    "## 3. Execution\n",
    "In this part, we execute the BMT model to generate captions for each of the samples in the `labc_path` dataframe. The results will then be taken to obtain the following metrics: \n",
    "- Captions Per Minute (CPM): the number of captions the BMT generates after running a single video, divided by the duration of the video in minutes.\n",
    "- Average number of captions\n",
    "- Average CPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd42939",
   "metadata": {},
   "source": [
    "### 3.1 Preparation\n",
    "\n",
    "Assign the path to a directory where you would like to create the output file. We name this file `4_3_bmt_captions.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output file\n",
    "OUTPUT_PATH = '/home/A01630791/bmt_clipping_experiments/Output_4_3'\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/4_3_bmt_captions.json', \"w\") as f:\n",
    "  f.write(\"[]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dd45e",
   "metadata": {},
   "source": [
    "The next file will be used for the output of the scene detection technique. We name the file `4_3_bmt_captions_ps.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output file\n",
    "OUTPUT_PATH = '/home/A01630791/bmt_clipping_experiments/Output_4_3'\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/4_3_bmt_captions_ps.json', \"w\") as f:\n",
    "  f.write(\"[]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f18c4d",
   "metadata": {},
   "source": [
    "### 3.3 BMT Installation (optional)\n",
    "\n",
    "Uncomment and run the following cell if you haven't installed the BMT model in experiment 4.1\n",
    "\n",
    "Run directly in terminal if the notebook throws errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00681c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Download BMT Checkpoints\n",
    "\n",
    "!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/glove.840B.300d.zip\n",
    "!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_cap_model.pt\n",
    "!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_prop_model.pt\n",
    "!wget https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "\n",
    "!mkdir .vector_cache\n",
    "!mv glove.840B.300d.zip ./.vector_cache/\n",
    "!mv best_cap_model.pt ./sample/\n",
    "!mv best_prop_model.pt ./sample/\n",
    "!mv vggish_model.ckpt ./submodules/video_features/models/vggish/checkpoints/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e78982",
   "metadata": {},
   "source": [
    "### 3.4 BMT Function\n",
    "The following cell generates I3D features, Vggish features, event proposals and event captions for each of the videos in `labc_df`. It wraps all this steps in one function used later for each clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb026b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_bmt(input_path, output_path, video_id, clip_id):\n",
    "    # captioning parameters\n",
    "    MAX_PROP_PER_VIDEO = 10\n",
    "    NMS_TIOU_THRESHOLD = 0.4\n",
    "\n",
    "    # checkpoint paths\n",
    "    PROPOSAL_CKPT = f'{WD}/sample/best_prop_model.pt'\n",
    "    CAPTIONING_CKPT = f'{WD}/sample/best_cap_model.pt'\n",
    "\n",
    "    # Extract visual and audio features\n",
    "\n",
    "    ## Prepare complementary paths\n",
    "    MY_VIDEO_PATH = input_path\n",
    "\n",
    "    VIDEO_DURATION = get_video_duration(MY_VIDEO_PATH)\n",
    "\n",
    "    FEATURES_CACHE_PATH = f'{WD}/tmp/'\n",
    "    FEATURES_PATH_STUB = os.path.join(FEATURES_CACHE_PATH, Path(MY_VIDEO_PATH).stem)\n",
    "    FEATURE_PATH_VGGISH = f'{FEATURES_PATH_STUB}_vggish.npy'\n",
    "    FEATURE_PATH_RGB = f'{FEATURES_PATH_STUB}_rgb.npy'\n",
    "    FEATURE_PATH_FLOW = f'{FEATURES_PATH_STUB}_flow.npy'\n",
    "\n",
    "    ## Extract I3D features\n",
    "    print('Extracting I3D features.')\n",
    "    !cd ./submodules/video_features && $ENVS_PATH/i3d/bin/python main.py \\\n",
    "      --feature_type i3d \\\n",
    "      --on_extraction save_numpy \\\n",
    "      --device_ids 0 \\\n",
    "      --extraction_fps 25 \\\n",
    "      --video_paths $MY_VIDEO_PATH \\\n",
    "      --output_path $FEATURES_CACHE_PATH\n",
    "\n",
    "    print('Extracting VGGish features.')\n",
    "    ## Extract VGGish features (audio)\n",
    "    !cd ./submodules/video_features && $ENVS_PATH/vggish/bin/python main.py \\\n",
    "      --feature_type vggish \\\n",
    "      --on_extraction save_numpy \\\n",
    "      --device_ids 0 \\\n",
    "      --video_paths $MY_VIDEO_PATH \\\n",
    "      --output_path $FEATURES_CACHE_PATH\n",
    "\n",
    "    # Step 3: Captioning\n",
    "    print('Generating caption...')\n",
    "\n",
    "    try:\n",
    "        ## Run single video prediction\n",
    "        !$ENVS_PATH/bmt/bin/python ./sample/single_video_prediction.py \\\n",
    "          --prop_generator_model_path $PROPOSAL_CKPT \\\n",
    "          --pretrained_cap_model_path $CAPTIONING_CKPT \\\n",
    "          --vggish_features_path $FEATURE_PATH_VGGISH \\\n",
    "          --rgb_features_path $FEATURE_PATH_RGB \\\n",
    "          --flow_features_path $FEATURE_PATH_FLOW \\\n",
    "          --duration_in_secs $VIDEO_DURATION \\\n",
    "          --device_id 0 \\\n",
    "          --max_prop_per_vid $MAX_PROP_PER_VIDEO \\\n",
    "          --nms_tiou_thresh $NMS_TIOU_THRESHOLD \\\n",
    "          --video_id $video_id \\\n",
    "          --clip_id $clip_id \\\n",
    "          --output_path $output_path\n",
    "    except AssertionError:\n",
    "        print('Exceeded MAX_PROP_PER_VIDEO')\n",
    "        exec_md['too_many_features'].append(video_id)\n",
    "\n",
    "    print('BMT Execution completed.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac36e6d",
   "metadata": {},
   "source": [
    "### 3.4 Fixed-time clipping\n",
    "For this first test, we will be using a window of 2 minutes for performing the fixed-time clipping. Assign the variable `FT_CLIPS_DIR` the path to the directory where you wish to store the clips temporarily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e377c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_CLIPS_DIR = f'{OUTPUT_PATH}/ft_clips' # Your path here\n",
    "%ls $FT_CLIPS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b0a31",
   "metadata": {},
   "source": [
    "The next cell instantiates the clipping module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78131a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ClippingModule(\n",
    "    path_to_video_out=FT_CLIPS_DIR, \n",
    "    window=120, \n",
    "    technique='fixed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e6c3b",
   "metadata": {},
   "source": [
    "We then obtain clips from all videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e88b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videos_list = os.listdir(LABC_VIDEOS_PATH)\n",
    "\n",
    "for video in videos_list[:26]:\n",
    "    if video[0] != '.': # ignore hidden files\n",
    "        cm.get_clips(f'{LABC_VIDEOS_PATH}/{video}', video[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf62bc",
   "metadata": {},
   "source": [
    "We need to create a DataFrame to keep track of all filenames, video IDs and clip IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e30b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "df = pd.DataFrame(os.listdir(FT_CLIPS_DIR), columns=['filename'])\n",
    "\n",
    "# Clean\n",
    "df.drop(df[df['filename'].str.startswith('.')].index, inplace=True)\n",
    "\n",
    "# Obtain clip id\n",
    "df['clip_id'] = [(int) (re.findall(\"(?<=@).*?(?=\\.)\", row['filename'])[0]) for index, row in df.iterrows()]\n",
    "\n",
    "# Obtain video id\n",
    "df['video_id'] = [(re.findall(\".*?(?=\\@)\", row['filename'])[0]) for index, row in df.iterrows()]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: select fewer videos if time limitations\n",
    "limited_sample_video_ids = df.video_id.unique()[:10]\n",
    "limited_sample_video_ids\n",
    "\n",
    "limited_df = df[df['video_id'].isin(limited_sample_video_ids)]\n",
    "limited_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3ae43",
   "metadata": {},
   "source": [
    "The next cell will execute the BMT model to generate captions for each clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99421793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execution metadata\n",
    "exec_md = {\n",
    "    'current_sample_num': 1,\n",
    "    'total_samples': limited_df.shape[0],\n",
    "}\n",
    "\n",
    "\n",
    "for index, sample in limited_df.iterrows():\n",
    "    \n",
    "    run_bmt(\n",
    "        input_path=f'{_DI}/{sample.filename}', \n",
    "        output_dir=OUTPUT_PATH, \n",
    "        video_id=sample.video_id, \n",
    "        clip_id=sample.clip_id\n",
    "    )\n",
    "    \n",
    "\n",
    "    curr_sample = exec_md['current_sample_num']\n",
    "    exec_md['current_sample_num'] += 1\n",
    "    total_samples = exec_md['total_samples']\n",
    "    print(f'\\n ***** Processed {curr_sample}/{total_samples} samples. *****\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Secure the file with a copy to prevent rewriting\n",
    "!cp $OUTPUT_PATH/4_3_bmt_captions.json $OUTPUT_PATH/4_3_bmt_captions_save.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8342b",
   "metadata": {},
   "source": [
    "**Analyzing Fixed-Timed Clipping results**\n",
    "\n",
    "The next cell will create a dataframe using the output from BMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ft_results = f'{OUTPUT_PATH}/4_3_bmt_captions_save.json'\n",
    "\n",
    "ft_captions_bmt = pd.read_json(path_to_ft_results, orient='records')\n",
    "# ft_captions_bmt.sort_values(by=\"clip_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09142a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of captions column\n",
    "ft_captions_bmt['number_captions'] = [len(sample.captions) for _, sample in ft_captions_bmt.iterrows()]\n",
    "ft_captions_bmt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c5b51",
   "metadata": {},
   "source": [
    "Average number of generated captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4066f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_captions_bmt.number_captions.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820535b",
   "metadata": {},
   "source": [
    "Average captions per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708db929",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_captions_bmt['CPM'] = [sample.number_captions / (sample.duration / 60) for _, sample in ft_captions_bmt.iterrows()]\n",
    "ft_captions_bmt.CPM.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16031e81",
   "metadata": {},
   "source": [
    "Plot number of captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SAVE_PATH = OUTPUT_PATH\n",
    "\n",
    "x = ft_captions_bmt.duration\n",
    "y = ft_captions_bmt.number_captions\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.xlabel(\"Duration (seconds)\")\n",
    "plt.ylabel(\"Number of generated captions\")\n",
    "\n",
    "\n",
    "plt.savefig(f'{PLOT_SAVE_PATH}/3_4_a.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdd35e",
   "metadata": {},
   "source": [
    "The plot shown above appears irregular because most videos have the same duration. Those samples that appear on the left hand side (smaller duration) were the remainings obtained from the end of each video. The other samples appear with a duration between 30 to 35 seconds. This happens because the BMT recalculates the duration based on a specific framerate used by the model. Hence, a video with an original framerate of 30 frames per second (fps) will be assigned a larger duration if the framerate used by BMT is 25 fps.\n",
    "\n",
    "A histogram will help better visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_captions_bmt.groupby(\"video_id\")[\"number_captions\"].count().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb30a",
   "metadata": {},
   "source": [
    "### 3.5 `pyscene` clipping\n",
    "For this first test, we will be using a window of 2 minutes for performing the fixed-time clipping. Assign the variable `FT_CLIPS_DIR` the path to the directory where you wish to store the clips temporarily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9209743",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABC_VIDEOS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS_CLIPS_DIR = f'{OUTPUT_PATH}/ps_clips' # Your path here\n",
    "%ls $PS_CLIPS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a426dc1",
   "metadata": {},
   "source": [
    "The next cell instantiates the clipping module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ClippingModule(\n",
    "    path_to_video_out=PS_CLIPS_DIR, \n",
    "    technique='scene'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bd616",
   "metadata": {},
   "source": [
    "We then obtain clips from all videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036c813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videos_list = os.listdir(LABC_VIDEOS_PATH)\n",
    "\n",
    "for video in videos_list[:10]:\n",
    "    if video[0] != '.': # ignore hidden files\n",
    "        cm.get_clips(input_path=f'{LABC_VIDEOS_PATH}/{video}', name=video[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b238b",
   "metadata": {},
   "source": [
    "We need to create a DataFrame to keep track of all filenames, video IDs and clip IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6308395e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DAlNWRjXY4A', '0RqBZlDur5k', '_VWAFXFRTcA', '0dyQouKDR2M',\n",
       "       '3oWlYHKMyv8', 'LlBedonOnR0', '3X7OqTDi8NQ', 'FJ64K9QdwDU'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "df = pd.DataFrame(os.listdir(PS_CLIPS_DIR), columns=['filename'])\n",
    "\n",
    "# Clean\n",
    "df.drop(df[df['filename'].str.startswith('.')].index, inplace=True)\n",
    "\n",
    "# Obtain clip id\n",
    "df['clip_id'] = [(int) (re.findall(\"(?<=@).*?(?=\\.)\", row['filename'])[0]) for index, row in df.iterrows()]\n",
    "\n",
    "# Obtain video id\n",
    "df['video_id'] = [(re.findall(\".*?(?=\\@)\", row['filename'])[0]) for index, row in df.iterrows()]\n",
    "\n",
    "df.video_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf895d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>clip_id</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DAlNWRjXY4A@017.mp4</td>\n",
       "      <td>17</td>\n",
       "      <td>DAlNWRjXY4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAlNWRjXY4A@018.mp4</td>\n",
       "      <td>18</td>\n",
       "      <td>DAlNWRjXY4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DAlNWRjXY4A@019.mp4</td>\n",
       "      <td>19</td>\n",
       "      <td>DAlNWRjXY4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAlNWRjXY4A@020.mp4</td>\n",
       "      <td>20</td>\n",
       "      <td>DAlNWRjXY4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAlNWRjXY4A@021.mp4</td>\n",
       "      <td>21</td>\n",
       "      <td>DAlNWRjXY4A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename  clip_id     video_id\n",
       "0  DAlNWRjXY4A@017.mp4       17  DAlNWRjXY4A\n",
       "1  DAlNWRjXY4A@018.mp4       18  DAlNWRjXY4A\n",
       "2  DAlNWRjXY4A@019.mp4       19  DAlNWRjXY4A\n",
       "3  DAlNWRjXY4A@020.mp4       20  DAlNWRjXY4A\n",
       "4  DAlNWRjXY4A@021.mp4       21  DAlNWRjXY4A"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional: select fewer videos if time limitations\n",
    "limited_sample_video_ids = df.video_id.unique()[:10]\n",
    "limited_sample_video_ids\n",
    "\n",
    "limited_df = df[df['video_id'].isin(limited_sample_video_ids)]\n",
    "limited_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f253a565",
   "metadata": {},
   "source": [
    "The next cell will execute the BMT model to generate captions for each clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671aae84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execution metadata\n",
    "exec_md = {\n",
    "    'current_sample_num': 1,\n",
    "    'total_samples': limited_df.shape[0],\n",
    "}\n",
    "\n",
    "\n",
    "for index, sample in limited_df.iterrows():\n",
    "    \n",
    "    run_bmt(\n",
    "        input_path=f'{PS_CLIPS_DIR}/{sample.filename}', \n",
    "        output_path=f'{OUTPUT_PATH}/4_3_bmt_captions_ps.json', \n",
    "        video_id=sample.video_id, \n",
    "        clip_id=sample.clip_id\n",
    "    )\n",
    "    \n",
    "\n",
    "    curr_sample = exec_md['current_sample_num']\n",
    "    exec_md['current_sample_num'] += 1\n",
    "    total_samples = exec_md['total_samples']\n",
    "    print(f'\\n ***** Processed {curr_sample}/{total_samples} samples. *****\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Secure the file with a copy to prevent rewriting\n",
    "!cp $OUTPUT_PATH/4_3_bmt_captions_ps.json $OUTPUT_PATH/4_3_bmt_captions_ps_save.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129d1c5",
   "metadata": {},
   "source": [
    "**Analyzing pyscene clipping results**\n",
    "\n",
    "The next cell will create a dataframe using the output from BMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ps_results = f'{OUTPUT_PATH}/4_3_bmt_captions_ps_save.json'\n",
    "\n",
    "ps_captions_bmt = pd.read_json(path_to_ps_results, orient='records')\n",
    "# ft_captions_bmt.sort_values(by=\"clip_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of captions column\n",
    "ps_captions_bmt['number_captions'] = [len(sample.captions) for _, sample in ps_captions_bmt.iterrows()]\n",
    "ps_captions_bmt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b6e33",
   "metadata": {},
   "source": [
    "Average number of generated captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff62a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_captions_bmt.number_captions.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9effe16",
   "metadata": {},
   "source": [
    "Average captions per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_captions_bmt['CPM'] = [sample.number_captions / (sample.duration / 60) for _, sample in ps_captions_bmt.iterrows()]\n",
    "ps_captions_bmt.CPM.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff2527",
   "metadata": {},
   "source": [
    "Plot number of captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SAVE_PATH = OUTPUT_PATH\n",
    "\n",
    "x = ps_captions_bmt.duration\n",
    "y = ps_captions_bmt.number_captions\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.xlabel(\"Duration (seconds)\")\n",
    "plt.ylabel(\"Number of generated captions\")\n",
    "\n",
    "\n",
    "plt.savefig(f'{PLOT_SAVE_PATH}/3_4_a.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8c8d8",
   "metadata": {},
   "source": [
    "The plot shown above appears irregular because most videos have the same duration. Those samples that appear on the left hand side (smaller duration) were the remainings obtained from the end of each video. The other samples appear with a duration between 30 to 35 seconds. This happens because the BMT recalculates the duration based on a specific framerate used by the model. Hence, a video with an original framerate of 30 frames per second (fps) will be assigned a larger duration if the framerate used by BMT is 25 fps.\n",
    "\n",
    "A histogram will help better visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_captions_bmt.groupby(\"video_id\")[\"number_captions\"].count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6afafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa11c594",
   "metadata": {},
   "source": [
    "**Comparisons with histograms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "anetc_val_path = '/home/A01630791/bmt_clipping_experiments/ActivityNet_Captions/val_1.json'\n",
    "anetc_val_df = pd.read_json(anetc_val_path, orient='index')\n",
    "\n",
    "bmt_results_path = '/home/A01630791/bmt_clipping_experiments/Output_4_1/4_1_bmt_captions_save.json'\n",
    "bmt_results_df = pd.read_json(bmt_results_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute no. captions\n",
    "anetc_val_df['number_captions'] = [len(sample.sentences) for index, sample in anetc_val_df.iterrows()]\n",
    "\n",
    "bmt_results_df['number_captions'] = [len(sample.captions) for index, sample in bmt_results_df.iterrows()]\n",
    "bmt_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c12a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs\n",
    "anetc_val_df.number_captions.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmt_results_df.number_captions.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi_fig = plt.figure(dpi=300)\n",
    "\n",
    "anetc_val_df.iloc[:].number_captions.plot.hist(color=\"lightblue\")\n",
    "ax = ft_captions_bmt.number_captions.plot.hist()\n",
    "ax.legend(['ActivityNet (Annotations)', 'LABC (BMT-generated)'])\n",
    "ax.set_xlabel('Number of captions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(f'{PLOT_SAVE_PATH}/3_4_b.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c111d5",
   "metadata": {},
   "source": [
    "The histogram shown above with number of captions against frequency displays the similarity between the distrbution of human annotations the model was trained on and the distribution of the number of captions generated by the BMT with a different dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43bc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt",
   "language": "python",
   "name": "bmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
