{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df4676d",
   "metadata": {},
   "source": [
    "# Experiment 4.2: Testing the BMT with unclipped videos from LABC\n",
    "\n",
    "The aim of this experiment is to find differences between the event proposals that are generated by processing the ActivityNet Captions dataset and the Long ActivityNet-Based Categories (LABC) dataset. This will help us understand whether the capacity of the BMT to detect events is affected by video duration. The results will be used in later experiments to determine if our solution improves the performance of the BMT when processing longer videos. **Experiment 4.1 must be executed before this experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686b00a",
   "metadata": {},
   "source": [
    "## 1. Enviornment\n",
    "\n",
    "In the following cell, change assign WD the path to the BMT-Clipping repository. \n",
    "Additionally, assign the path to your environments directory (e.g. where conda stores all directories) to the ENVS_PATH variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0631453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/A01630791/BMT-Clipping\n"
     ]
    }
   ],
   "source": [
    "# Working directory (it must be the repository's root directory)\n",
    "WD = '/home/A01630791/BMT-Clipping'\n",
    "%cd $WD\n",
    "\n",
    "# Environments directory (e.g. anaconda3/envs)\n",
    "ENVS_PATH = '/home/A01630791/anaconda3/envs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0a27c",
   "metadata": {},
   "source": [
    "**Optional**  \n",
    "Uncomment and run the following cell if you haven't configured the Python environment in experiment 4.1\n",
    "\n",
    "Run directly in terminal if the notebook throws errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704b8916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Environment\\n## feature extraction (run directly in terminal if the notebook can't execute)\\n!conda env create -f ./submodules/video_features/conda_env_i3d.yml\\n!conda env create -f ./submodules/video_features/conda_env_vggish.yml\\n## captioning model (run directly in terminal if the notebook can't execute)\\n!conda env create -f ./conda_env.yml\\n## spacy language model (use the path to your 'bmt' environment instead)\\n!$ENVS_PATH/bmt/bin/python -m spacy download en\\n\\n\\n# (Optional) Install additional libraries in environment (run directly in terminal if the notebook can't execute)\\n!conda install pytube\\n!conda install numpy\\n!conda install matplotlib\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Environment\n",
    "## feature extraction (run directly in terminal if the notebook can't execute)\n",
    "!conda env create -f ./submodules/video_features/conda_env_i3d.yml\n",
    "!conda env create -f ./submodules/video_features/conda_env_vggish.yml\n",
    "## captioning model (run directly in terminal if the notebook can't execute)\n",
    "!conda env create -f ./conda_env.yml\n",
    "## spacy language model (use the path to your 'bmt' environment instead)\n",
    "!$ENVS_PATH/bmt/bin/python -m spacy download en\n",
    "\n",
    "\n",
    "# (Optional) Install additional libraries in environment (run directly in terminal if the notebook can't execute)\n",
    "!conda install pytube\n",
    "!conda install numpy\n",
    "!conda install matplotlib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5627db0",
   "metadata": {},
   "source": [
    "**BMT imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e8a84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(WD)\n",
    "from sample.single_video_prediction import get_video_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc33cc0",
   "metadata": {},
   "source": [
    "**Other imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d323b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e8c7b",
   "metadata": {},
   "source": [
    "## 2. Importing the dataset\n",
    "\n",
    "A copy of LABC is provided in our repository in `data/long_activitynet_categories.csv`, so there is no need to download it. The url is https://github.com/oscarmires/BMT-Clipping/blob/master/data/long_activitynet_categories.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c075072c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>duration</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0dyQouKDR2M</th>\n",
       "      <td>https://www.youtube.com/watch?v=0dyQouKDR2M</td>\n",
       "      <td>1169</td>\n",
       "      <td>Archery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LjGCKmNgMho</th>\n",
       "      <td>https://www.youtube.com/watch?v=LjGCKmNgMho</td>\n",
       "      <td>1822</td>\n",
       "      <td>Ballet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cQlk39LS42M</th>\n",
       "      <td>https://www.youtube.com/watch?v=cQlk39LS42M</td>\n",
       "      <td>2791</td>\n",
       "      <td>Basketball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dOHny8s8iBI</th>\n",
       "      <td>https://www.youtube.com/watch?v=dOHny8s8iBI</td>\n",
       "      <td>1323</td>\n",
       "      <td>Bathing dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k4jLxVILsFE</th>\n",
       "      <td>https://www.youtube.com/watch?v=k4jLxVILsFE</td>\n",
       "      <td>3391</td>\n",
       "      <td>Clean and jerk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  duration  \\\n",
       "id                                                                   \n",
       "0dyQouKDR2M  https://www.youtube.com/watch?v=0dyQouKDR2M      1169   \n",
       "LjGCKmNgMho  https://www.youtube.com/watch?v=LjGCKmNgMho      1822   \n",
       "cQlk39LS42M  https://www.youtube.com/watch?v=cQlk39LS42M      2791   \n",
       "dOHny8s8iBI  https://www.youtube.com/watch?v=dOHny8s8iBI      1323   \n",
       "k4jLxVILsFE  https://www.youtube.com/watch?v=k4jLxVILsFE      3391   \n",
       "\n",
       "                   category  \n",
       "id                           \n",
       "0dyQouKDR2M         Archery  \n",
       "LjGCKmNgMho          Ballet  \n",
       "cQlk39LS42M      Basketball  \n",
       "dOHny8s8iBI     Bathing dog  \n",
       "k4jLxVILsFE  Clean and jerk  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the value of the following variable with the path to the dataset in your fs.\n",
    "LABC_PATH = f'{WD}/data/long_activitynet_categories.csv'\n",
    "LABC_PATH\n",
    "\n",
    "# Open the file as a Pandas DF\n",
    "labc_df = pd.read_csv(LABC_PATH, index_col=0)\n",
    "labc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33538199",
   "metadata": {},
   "source": [
    "## 3. Preparing and pre-analyzing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478b8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d71156d5",
   "metadata": {},
   "source": [
    "## 4. Execution\n",
    "In this part, we execute the BMT model to generate captions for each of the samples in the `labc_path` dataframe. The results will then be taken to obtain the following metrics: \n",
    "- Captions Per Minute (CPM): the number of captions the BMT generates after running a single video, divided by the duration of the video in minutes.\n",
    "- Average number of captions\n",
    "- Average CPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd42939",
   "metadata": {},
   "source": [
    "### 4.1 Preparation\n",
    "\n",
    "Assign the path where you would like to create the output file. We name this file `4_2_bmt_captions.json`.\n",
    "\n",
    "Additionally, create a directory in your filesystem to temporarily store the YouTube videos. Assign `LABC_VIDEOS_PATH` the path to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5c4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output file\n",
    "OUTPUT_PATH = '/home/A01630791/bmt_clipping_experiments/Output_4_2'\n",
    "\n",
    "with open(f'{OUTPUT_PATH}/4_2_bmt_captions.json', \"w\") as f:\n",
    "  f.write(\"[]\")\n",
    "\n",
    "# Video directory (path to your)\n",
    "LABC_VIDEOS_PATH = '/home/A01630791/bmt_clipping_experiments/LABC_videos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f18c4d",
   "metadata": {},
   "source": [
    "### 4.2 BMT Installation (optional)\n",
    "\n",
    "Uncomment and run the following cell if you haven't installed the BMT model in experiment 4.1\n",
    "\n",
    "Run directly in terminal if the notebook throws errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00681c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Download BMT Checkpoints\\n\\n!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/glove.840B.300d.zip\\n!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_cap_model.pt\\n!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_prop_model.pt\\n!wget https://storage.googleapis.com/audioset/vggish_model.ckpt\\n\\n!mkdir .vector_cache\\n!mv glove.840B.300d.zip ./.vector_cache/\\n!mv best_cap_model.pt ./sample/\\n!mv best_prop_model.pt ./sample/\\n!mv vggish_model.ckpt ./submodules/video_features/models/vggish/checkpoints/\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Download BMT Checkpoints\n",
    "\n",
    "!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/glove.840B.300d.zip\n",
    "!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_cap_model.pt\n",
    "!wget https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/bmt/best_prop_model.pt\n",
    "!wget https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
    "\n",
    "!mkdir .vector_cache\n",
    "!mv glove.840B.300d.zip ./.vector_cache/\n",
    "!mv best_cap_model.pt ./sample/\n",
    "!mv best_prop_model.pt ./sample/\n",
    "!mv vggish_model.ckpt ./submodules/video_features/models/vggish/checkpoints/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e78982",
   "metadata": {},
   "source": [
    "### 4.3 BMT Processing\n",
    "The following cell generates I3D features, Vggish features, event proposals and event captions for each of the videos in `labc_df`. The execution might last for several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb026b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample download complete.\n",
      "Video Duration: 1168.102969\n",
      "Generating caption.\n",
      "Contructing caption_iterator for \"train\" phase\n",
      "Using vanilla Generator\n",
      "initialization: xavier\n",
      "Glove emb of the same size as d_model_caps\n",
      "Pretrained caption path: \n",
      " /home/A01630791/BMT-Clipping/sample/best_cap_model.pt\n",
      "***** S *****:  1216\n",
      "Traceback (most recent call last):\n",
      "  File \"./sample/single_video_prediction.py\", line 308, in <module>\n",
      "    prop_model, feature_paths, train_dataset.pad_idx, prop_cfg, args.device_id, args.duration_in_secs\n",
      "  File \"./sample/single_video_prediction.py\", line 170, in generate_proposals\n",
      "    pad_feats_up_to=cfg.pad_feats_up_to\n",
      "  File \"./sample/single_video_prediction.py\", line 63, in load_features_from_npy\n",
      "    stack_vggish = pad_segment(stack_vggish, pad_feats_up_to['audio'], pad_idx)\n",
      "  File \"/home/A01630791/BMT-Clipping/sample/../datasets/load_features.py\", line 40, in pad_segment\n",
      "    assert S <= max_feature_len\n",
      "AssertionError\n",
      "Sample deleted\n",
      "\n",
      " ***** Processed 1/50 samples. *****\n",
      "\n",
      "Sample download complete.\n",
      "Video Duration: 1821.071375\n",
      "Generating caption.\n",
      "Contructing caption_iterator for \"train\" phase\n",
      "Using vanilla Generator\n",
      "initialization: xavier\n",
      "Glove emb of the same size as d_model_caps\n",
      "Pretrained caption path: \n",
      " /home/A01630791/BMT-Clipping/sample/best_cap_model.pt\n",
      "Traceback (most recent call last):\n",
      "  File \"./sample/single_video_prediction.py\", line 308, in <module>\n",
      "    prop_model, feature_paths, train_dataset.pad_idx, prop_cfg, args.device_id, args.duration_in_secs\n",
      "  File \"./sample/single_video_prediction.py\", line 170, in generate_proposals\n",
      "    pad_feats_up_to=cfg.pad_feats_up_to\n",
      "  File \"./sample/single_video_prediction.py\", line 53, in load_features_from_npy\n",
      "    stack_vggish = np.load(feature_paths['audio'])\n",
      "  File \"/home/A01630791/anaconda3/envs/bmt/lib/python3.7/site-packages/numpy/lib/npyio.py\", line 384, in load\n",
      "    fid = open(file, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/A01630791/BMT-Clipping/tmp/LjGCKmNgMho_vggish.npy'\n",
      "Sample deleted\n",
      "\n",
      " ***** Processed 2/50 samples. *****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# captioning parameters\n",
    "MAX_PROP_PER_VIDEO = 10\n",
    "NMS_TIOU_THRESHOLD = 0.4\n",
    "\n",
    "# checkpoint paths\n",
    "PROPOSAL_CKPT = f'{WD}/sample/best_prop_model.pt'\n",
    "CAPTIONING_CKPT = f'{WD}/sample/best_cap_model.pt'\n",
    "\n",
    "# execution metadata\n",
    "exec_md = {\n",
    "    'not_found_videos': [],\n",
    "    'current_sample_num': 1,\n",
    "    'total_samples': labc_df.shape[0],\n",
    "    'too_many_features': []\n",
    "}\n",
    "\n",
    "for video_id, sample in labc_df.iterrows():\n",
    "\n",
    "    # Step 1: Download the video\n",
    "    try: \n",
    "        \n",
    "        yt = YouTube(sample['url']) \n",
    "        \n",
    "        filename = f'{video_id}.mp4'\n",
    "        stream = yt.streams.get_highest_resolution()\n",
    "        stream.download(output_path=LABC_VIDEOS_PATH, filename=filename)\n",
    "        print('Sample download complete.')\n",
    "        \n",
    "    except: \n",
    "        print(\"Error: can't instantiate YouTube object.\") \n",
    "        exec_md['not_found_videos'].append(video_id)\n",
    "        \n",
    "    # Step 2: Extract visual and audio features\n",
    "\n",
    "    ## Prepare complementary paths\n",
    "    MY_VIDEO_PATH = f'{LABC_VIDEOS_PATH}/{video_id}.mp4'\n",
    "\n",
    "    VIDEO_DURATION = get_video_duration(MY_VIDEO_PATH)\n",
    "\n",
    "    FEATURES_CACHE_PATH = f'{WD}/tmp/'\n",
    "    FEATURES_PATH_STUB = os.path.join(FEATURES_CACHE_PATH, Path(MY_VIDEO_PATH).stem)\n",
    "    FEATURE_PATH_VGGISH = f'{FEATURES_PATH_STUB}_vggish.npy'\n",
    "    FEATURE_PATH_RGB = f'{FEATURES_PATH_STUB}_rgb.npy'\n",
    "    FEATURE_PATH_FLOW = f'{FEATURES_PATH_STUB}_flow.npy'\n",
    "\n",
    "    \"\"\"\n",
    "    ## Extract I3D features\n",
    "    print('Extracting I3D features.')\n",
    "    !cd ./submodules/video_features && $ENVS_PATH/i3d/bin/python main.py \\\n",
    "      --feature_type i3d \\\n",
    "      --on_extraction save_numpy \\\n",
    "      --device_ids 0 \\\n",
    "      --extraction_fps 25 \\\n",
    "      --video_paths $MY_VIDEO_PATH \\\n",
    "      --output_path $FEATURES_CACHE_PATH\n",
    "\n",
    "    print('Extracting VGGish features.')\n",
    "    ## Extract VGGish features (audio)\n",
    "    !cd ./submodules/video_features && $ENVS_PATH/vggish/bin/python main.py \\\n",
    "      --feature_type vggish \\\n",
    "      --on_extraction save_numpy \\\n",
    "      --device_ids 0 \\\n",
    "      --video_paths $MY_VIDEO_PATH \\\n",
    "      --output_path $FEATURES_CACHE_PATH\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Captioning\n",
    "    print('Generating caption.')\n",
    "    \n",
    "    try:\n",
    "        ## Run single video prediction\n",
    "        !$ENVS_PATH/bmt/bin/python ./sample/single_video_prediction.py \\\n",
    "          --prop_generator_model_path $PROPOSAL_CKPT \\\n",
    "          --pretrained_cap_model_path $CAPTIONING_CKPT \\\n",
    "          --vggish_features_path $FEATURE_PATH_VGGISH \\\n",
    "          --rgb_features_path $FEATURE_PATH_RGB \\\n",
    "          --flow_features_path $FEATURE_PATH_FLOW \\\n",
    "          --duration_in_secs $VIDEO_DURATION \\\n",
    "          --device_id 0 \\\n",
    "          --max_prop_per_vid $MAX_PROP_PER_VIDEO \\\n",
    "          --nms_tiou_thresh $NMS_TIOU_THRESHOLD \\\n",
    "          --video_id $video_id \\\n",
    "          --output_path $OUTPUT_PATH/4_1_bmt_captions.json\n",
    "    except AssertionError:\n",
    "        print('Exceeded MAX_PROP_PER_VIDEO')\n",
    "        exec_md['too_many_features'].append(video_id)\n",
    "\n",
    "    # Step 4: Delete video to free space\n",
    "    os.remove(MY_VIDEO_PATH)\n",
    "    print('Sample deleted')\n",
    "\n",
    "        \n",
    "    curr_sample = exec_md['current_sample_num']\n",
    "    exec_md['current_sample_num'] += 1\n",
    "    total_samples = exec_md['total_samples']\n",
    "    print(f'\\n ***** Processed {curr_sample}/{total_samples} samples. *****\\n')\n",
    "    \n",
    "\n",
    "print('Execution completed.') \n",
    "print(exec_md)\n",
    "print('YouTube not found videos:', len(exec_md['not_found_videos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d3347",
   "metadata": {},
   "source": [
    "**AssertionError**\n",
    "As we can see in the execution of the last cell, a very long video (>10 minutes) causes an AssertionError exception to be raised. This originates when the model finds that the number of features found in the video exceeds a limit established in the training configuration. In other words, the model can't process larger videos than the maximum duration that was determined during training. Nonetheless, we continue our experiment with videos with a duration ranging from 4 to 10 minutes, still under ActivityNet categories. This means we will be analysing videos that are not as long as we wanted, but still longer than ActivityNet Caption's videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f066ad",
   "metadata": {},
   "source": [
    "### 4.4 Medium-sized ActivityNet Categories\n",
    "We gathered a dataset of 10 videos under ActivityNet Categories and fed them to the BMT model. We provide the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9d3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt",
   "language": "python",
   "name": "bmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
